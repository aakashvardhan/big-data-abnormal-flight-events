{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Feature Engineering\n",
        "\n",
        "This notebook extracts comprehensive features from the processed flight data for anomaly detection.\n",
        "\n",
        "## Objectives\n",
        "1. Extract temporal features (duration, phase times, taxi times)\n",
        "2. Extract spatial features (distance, trajectory sinuosity, altitude patterns)\n",
        "3. Extract operational features (runway/taxiway events, ground complexity)\n",
        "4. Extract sequence features (event n-grams, state transitions)\n",
        "5. Extract contextual features (airport norms, peer comparisons)\n",
        "6. Validate and analyze feature distributions\n",
        "7. Prepare features for machine learning\n",
        "\n",
        "## Prerequisites\n",
        "- Phase 1 must be completed\n",
        "- Processed data files must exist in `data/processed/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported successfully\n",
            "Project root: c:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\n"
          ]
        }
      ],
      "source": [
        "# Setup and Import Libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import feature engineering modules\n",
        "from src.features import (\n",
        "    load_processed_data,\n",
        "    extract_all_features,\n",
        "    prepare_features_for_ml,\n",
        "    save_features,\n",
        "    run_feature_engineering_pipeline\n",
        ")\n",
        "from src.utils.helpers import load_config, ensure_dir\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded\n",
            "Figures will be saved to: c:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\outputs\\figures\n"
          ]
        }
      ],
      "source": [
        "# Configuration and Directory Setup\n",
        "config = load_config(project_root / 'config' / 'config.yaml')\n",
        "\n",
        "# Setup directories\n",
        "output_dir = project_root / config['output']['figures_dir']\n",
        "ensure_dir(output_dir)\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"Figures will be saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Processed Data\n",
        "\n",
        "Load the processed data from Phase 1 (events and flight summaries).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading events from c:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\data\\processed\\events_sorted.csv.gz...\n",
            "✓ Loaded 9,554,720 events\n",
            "Loading flight summary from c:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\data\\processed\\flight_summary.csv.gz...\n",
            "✓ Loaded 3,833,918 flights\n",
            "\n",
            "Data Summary:\n",
            "  Events: 9,554,720 records\n",
            "  Flights: 3,833,918 unique flights\n",
            "\n",
            "Event columns: ['YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_NM', 'DEST', 'DEST_CITY_NAME', 'DEST_STATE_NM', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'DUP', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'DIV_AIRPORT_LANDINGS', 'DIV_REACHED_DEST', 'ORIGIN_STATE_ABR', 'DEST_STATE_ABR', 'flight_id']\n",
            "\n",
            "Flight summary columns: ['flight_id', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'DISTANCE', 'DEP_DELAY', 'ARR_DELAY']\n"
          ]
        }
      ],
      "source": [
        "# Load processed data from Phase 1\n",
        "processed_dir = project_root / config['data']['processed_dir']\n",
        "\n",
        "try:\n",
        "    events_df, flight_summary_df = load_processed_data(str(processed_dir))\n",
        "    \n",
        "    print(f\"\\nData Summary:\")\n",
        "    print(f\"  Events: {len(events_df):,} records\")\n",
        "    print(f\"  Flights: {len(flight_summary_df):,} unique flights\")\n",
        "    print(f\"\\nEvent columns: {list(events_df.columns)}\")\n",
        "    print(f\"\\nFlight summary columns: {list(flight_summary_df.columns)}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error: {e}\")\n",
        "    print(\"\\nPlease run Phase 1 notebook first to generate processed data.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract All Features\n",
        "\n",
        "Extract features from all five categories:\n",
        "1. **Temporal Features**: Duration, phase times, taxi times, time-of-day\n",
        "2. **Spatial Features**: Distance, trajectory sinuosity, altitude patterns\n",
        "3. **Operational Features**: Runway/taxiway events, ground complexity\n",
        "4. **Sequence Features**: Event n-grams, state transitions\n",
        "5. **Contextual Features**: Airport norms, peer comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE ENGINEERING PIPELINE\n",
            "============================================================\n",
            "\n",
            "[1/5] Extracting temporal features...\n",
            "No 'timestamp' column found in events_df; skipping temporal feature extraction.\n",
            "\n",
            "[2/5] Extracting spatial features...\n",
            "Extracting spatial features for all flights...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract all features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# This may take 10-30 minutes depending on data size\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m features_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_all_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevents_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflight_summary_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None = extract all feature types\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFeature extraction complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features_df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\src\\features\\feature_engineering.py:100\u001b[0m, in \u001b[0;36mextract_all_features\u001b[1;34m(events_df, flight_summary_df, feature_types)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m feature_types:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/5] Extracting spatial features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 100\u001b[0m     features_df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_spatial_features_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperational\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m feature_types:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[3/5] Extracting operational features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\src\\features\\spatial_features.py:299\u001b[0m, in \u001b[0;36mextract_spatial_features_batch\u001b[1;34m(flight_summary_df, events_df)\u001b[0m\n\u001b[0;32m    296\u001b[0m spatial_features_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flight_id \u001b[38;5;129;01min\u001b[39;00m flight_summary_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflight_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m--> 299\u001b[0m     flight_events \u001b[38;5;241m=\u001b[39m events_df[\u001b[43mevents_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflight_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mflight_id\u001b[49m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flight_events) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m         features \u001b[38;5;241m=\u001b[39m extract_spatial_features(flight_events)\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\venv\\lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\venv\\lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\venv\\lib\\site-packages\\pandas\\core\\series.py:5803\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5800\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   5801\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 5803\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\aiish\\OneDrive\\Desktop\\MSDA-SJSU\\Fall 2025\\Big Data\\project\\venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:132\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Extract all features\n",
        "# This may take 10-30 minutes depending on data size\n",
        "features_df = extract_all_features(\n",
        "    events_df,\n",
        "    flight_summary_df,\n",
        "    feature_types=None  # None = extract all feature types\n",
        ")\n",
        "\n",
        "print(f\"\\nFeature extraction complete!\")\n",
        "print(f\"Total features: {len(features_df.columns)}\")\n",
        "print(f\"Total flights: {len(features_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Feature Overview\n",
        "\n",
        "Examine the extracted features and their basic statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display feature overview\n",
        "print(\"Feature Overview:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total features: {len(features_df.columns)}\")\n",
        "print(f\"Total flights: {len(features_df)}\")\n",
        "print(f\"\\nFeature categories:\")\n",
        "\n",
        "# Categorize features\n",
        "feature_categories = {\n",
        "    'Temporal': [col for col in features_df.columns if any(x in col.lower() for x in ['duration', 'time', 'hour', 'day', 'phase', 'taxi', 'ground'])],\n",
        "    'Spatial': [col for col in features_df.columns if any(x in col.lower() for x in ['distance', 'lat', 'lon', 'trajectory', 'altitude', 'coordinate'])],\n",
        "    'Operational': [col for col in features_df.columns if any(x in col.lower() for x in ['runway', 'taxiway', 'parking', 'ground_complexity', 'event_count'])],\n",
        "    'Sequence': [col for col in features_df.columns if any(x in col.lower() for x in ['gram', 'transition', 'sequence', 'pattern'])],\n",
        "    'Contextual': [col for col in features_df.columns if any(x in col.lower() for x in ['zscore', 'deviation', 'percentile', 'airport', 'global', 'time_deviation'])]\n",
        "}\n",
        "\n",
        "for category, cols in feature_categories.items():\n",
        "    print(f\"  {category}: {len(cols)} features\")\n",
        "\n",
        "print(f\"\\nFirst few features:\")\n",
        "print(features_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Statistics and Missing Values\n",
        "\n",
        "Analyze feature distributions and handle missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_stats = features_df.isnull().sum()\n",
        "missing_stats = missing_stats[missing_stats > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing_stats) > 0:\n",
        "    print(\"Missing Values:\")\n",
        "    print(\"=\"*60)\n",
        "    print(missing_stats)\n",
        "    print(f\"\\nTotal missing values: {missing_stats.sum()}\")\n",
        "    print(f\"Features with missing values: {len(missing_stats)}\")\n",
        "    \n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    missing_stats.head(20).plot(kind='barh')\n",
        "    plt.title('Top 20 Features with Missing Values')\n",
        "    plt.xlabel('Missing Count')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_dir / 'feature_missing_values.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"✓ No missing values found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics for numeric features\n",
        "numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
        "print(f\"Numeric features: {len(numeric_cols)}\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nSummary Statistics (sample of key features):\")\n",
        "key_features = [\n",
        "    'total_duration_seconds', 'n_events', 'total_distance_meters',\n",
        "    'altitude_max', 'ground_complexity_score', 'sequence_complexity_score',\n",
        "    'global_zscore_duration', 'airport_zscore_duration'\n",
        "]\n",
        "\n",
        "available_key_features = [f for f in key_features if f in features_df.columns]\n",
        "if available_key_features:\n",
        "    print(features_df[available_key_features].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Feature Distributions\n",
        "\n",
        "Visualize distributions of key features to understand their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions of key temporal features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Duration distribution\n",
        "if 'total_duration_seconds' in features_df.columns:\n",
        "    axes[0, 0].hist(features_df['total_duration_seconds'] / 3600, bins=50, edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Duration (hours)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].set_title('Flight Duration Distribution')\n",
        "    axes[0, 0].set_xlim(0, features_df['total_duration_seconds'].quantile(0.99) / 3600)\n",
        "\n",
        "# Number of events\n",
        "if 'n_events' in features_df.columns:\n",
        "    axes[0, 1].hist(features_df['n_events'], bins=50, edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('Number of Events')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('Events per Flight Distribution')\n",
        "    axes[0, 1].set_xlim(0, features_df['n_events'].quantile(0.99))\n",
        "\n",
        "# Ground time\n",
        "if 'ground_time_seconds' in features_df.columns:\n",
        "    axes[1, 0].hist(features_df['ground_time_seconds'] / 60, bins=50, edgecolor='black')\n",
        "    axes[1, 0].set_xlabel('Ground Time (minutes)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Ground Time Distribution')\n",
        "    axes[1, 0].set_xlim(0, features_df['ground_time_seconds'].quantile(0.99) / 60)\n",
        "\n",
        "# Taxi time\n",
        "if 'taxi_time_seconds' in features_df.columns:\n",
        "    axes[1, 1].hist(features_df['taxi_time_seconds'] / 60, bins=50, edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Taxi Time (minutes)')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Taxi Time Distribution')\n",
        "    axes[1, 1].set_xlim(0, features_df['taxi_time_seconds'].quantile(0.99) / 60)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'feature_temporal_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions of key spatial features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Total distance\n",
        "if 'total_distance_meters' in features_df.columns:\n",
        "    axes[0, 0].hist(features_df['total_distance_meters'] / 1000, bins=50, edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Total Distance (km)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].set_title('Total Distance Distribution')\n",
        "    axes[0, 0].set_xlim(0, features_df['total_distance_meters'].quantile(0.99) / 1000)\n",
        "\n",
        "# Max altitude\n",
        "if 'altitude_max' in features_df.columns:\n",
        "    axes[0, 1].hist(features_df['altitude_max'] / 1000, bins=50, edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('Max Altitude (km)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('Maximum Altitude Distribution')\n",
        "    axes[0, 1].set_xlim(0, features_df['altitude_max'].quantile(0.99) / 1000)\n",
        "\n",
        "# Trajectory sinuosity\n",
        "if 'trajectory_sinuosity' in features_df.columns:\n",
        "    axes[1, 0].hist(features_df['trajectory_sinuosity'], bins=50, edgecolor='black')\n",
        "    axes[1, 0].set_xlabel('Trajectory Sinuosity')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Trajectory Sinuosity Distribution')\n",
        "    axes[1, 0].set_xlim(1, features_df['trajectory_sinuosity'].quantile(0.99))\n",
        "\n",
        "# Path efficiency\n",
        "if 'path_efficiency' in features_df.columns:\n",
        "    axes[1, 1].hist(features_df['path_efficiency'], bins=50, edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Path Efficiency')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Path Efficiency Distribution')\n",
        "    axes[1, 1].set_xlim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'feature_spatial_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Feature Correlation Analysis\n",
        "\n",
        "Analyze correlations between features to identify redundant or highly correlated features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for numeric features\n",
        "numeric_features = features_df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = features_df[numeric_features].corr()\n",
        "\n",
        "# Find highly correlated feature pairs\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.8:  # Threshold for high correlation\n",
        "            high_corr_pairs.append((\n",
        "                correlation_matrix.columns[i],\n",
        "                correlation_matrix.columns[j],\n",
        "                corr_val\n",
        "            ))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(f\"Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.8):\")\n",
        "    print(\"=\"*60)\n",
        "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:20]:\n",
        "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(\"✓ No highly correlated feature pairs found (|r| > 0.8)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize correlation matrix for key features\n",
        "key_features_for_corr = [\n",
        "    'total_duration_seconds', 'n_events', 'total_distance_meters',\n",
        "    'altitude_max', 'ground_complexity_score', 'sequence_complexity_score',\n",
        "    'global_zscore_duration', 'airport_zscore_duration', 'trajectory_sinuosity'\n",
        "]\n",
        "\n",
        "available_corr_features = [f for f in key_features_for_corr if f in features_df.columns]\n",
        "\n",
        "if len(available_corr_features) > 1:\n",
        "    corr_subset = features_df[available_corr_features].corr()\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Feature Correlation Matrix (Key Features)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_dir / 'feature_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Prepare Features for Machine Learning\n",
        "\n",
        "Handle missing values, encode categorical variables, and prepare features for ML models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for ML\n",
        "X, y, feature_info = prepare_features_for_ml(features_df)\n",
        "\n",
        "print(f\"\\nML-Ready Features:\")\n",
        "print(f\"  Shape: {X.shape}\")\n",
        "print(f\"  Feature names: {len(feature_info['feature_names'])}\")\n",
        "print(f\"  Samples: {len(X)}\")\n",
        "\n",
        "# Display sample of prepared features\n",
        "print(f\"\\nSample features (first 5 rows, first 10 columns):\")\n",
        "print(X.iloc[:5, :10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Features\n",
        "\n",
        "Save the extracted features for use in Phase 3 (Model Development).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save features\n",
        "features_dir = project_root / config['data']['features_dir']\n",
        "ensure_dir(features_dir)\n",
        "\n",
        "output_path = features_dir / 'flight_features.csv.gz'\n",
        "save_features(features_df, str(output_path), X, feature_info)\n",
        "\n",
        "print(f\"\\n✓ Features saved successfully!\")\n",
        "print(f\"  Full features: {output_path}\")\n",
        "print(f\"  ML-ready features: {features_dir / 'flight_features_ml_ready.csv.gz'}\")\n",
        "print(f\"  Feature info: {features_dir / 'feature_info.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Feature Summary Report\n",
        "\n",
        "Generate a summary report of all extracted features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate feature summary report\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING SUMMARY REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. Dataset Overview:\")\n",
        "print(f\"   - Total flights: {len(features_df):,}\")\n",
        "print(f\"   - Total features extracted: {len(features_df.columns)}\")\n",
        "print(f\"   - ML-ready features: {len(X.columns)}\")\n",
        "\n",
        "print(f\"\\n2. Feature Categories:\")\n",
        "for category, cols in feature_categories.items():\n",
        "    print(f\"   - {category}: {len(cols)} features\")\n",
        "\n",
        "print(f\"\\n3. Data Quality:\")\n",
        "print(f\"   - Missing values: {features_df.isnull().sum().sum()}\")\n",
        "print(f\"   - Features with missing values: {len(missing_stats)}\")\n",
        "print(f\"   - Numeric features: {len(numeric_cols)}\")\n",
        "print(f\"   - Categorical features: {len(features_df.select_dtypes(include=['object', 'category']).columns)}\")\n",
        "\n",
        "print(f\"\\n4. Key Statistics:\")\n",
        "if 'total_duration_seconds' in features_df.columns:\n",
        "    print(f\"   - Mean flight duration: {features_df['total_duration_seconds'].mean() / 3600:.2f} hours\")\n",
        "if 'n_events' in features_df.columns:\n",
        "    print(f\"   - Mean events per flight: {features_df['n_events'].mean():.1f}\")\n",
        "if 'total_distance_meters' in features_df.columns:\n",
        "    print(f\"   - Mean distance: {features_df['total_distance_meters'].mean() / 1000:.2f} km\")\n",
        "\n",
        "print(f\"\\n5. Next Steps:\")\n",
        "print(f\"   - Features are ready for Phase 3: Model Development\")\n",
        "print(f\"   - Use isolation forest, one-class SVM, or other anomaly detection models\")\n",
        "print(f\"   - Features saved to: {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2 COMPLETE! ✓\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
