{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Arrival Delay Classification (ARR_DEL15)\n",
        "\n",
        "This notebook builds supervised ML models to predict `ARR_DEL15`, which indicates whether an arrival delay was 15 minutes or more, using the airline on-time performance dataset prepared in Phase 1.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Load the processed dataset from Phase 1.\n",
        "2. Engineer features by selecting numerical columns and encoding categorical variables.\n",
        "3. Split the data into train and test sets.\n",
        "4. Train and evaluate classification models (Logistic Regression and Random Forest).\n",
        "\n",
        "To keep runtime reasonable on a large dataset (~9.5M rows), we train on a stratified sample of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "# Ensure project root is on path\n",
        "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.utils.helpers import load_config, ensure_dir\n",
        "\n",
        "print(\"Project root:\", project_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration and processed data from Phase 1\n",
        "config = load_config(project_root / 'config' / 'config.yaml')\n",
        "\n",
        "processed_dir = project_root / config['data']['processed_dir']\n",
        "print(\"Processed data directory:\", processed_dir)\n",
        "\n",
        "# Load the processed events data (one row per flight in this project setup)\n",
        "events_path = processed_dir / 'events_sorted.csv.gz'\n",
        "print(\"Loading data from:\", events_path)\n",
        "\n",
        "df = pd.read_csv(events_path, compression='gzip')\n",
        "print(f\"Loaded {len(df):,} rows and {df.shape[1]} columns\")\n",
        "print(\"Columns:\", list(df.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare target variable ARR_DEL15\n",
        "TARGET_COL = 'ARR_DEL15'\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise KeyError(f\"Target column {TARGET_COL} not found in dataset\")\n",
        "\n",
        "# Drop rows with missing target\n",
        "initial_rows = len(df)\n",
        "df = df[df[TARGET_COL].notna()].copy()\n",
        "print(f\"Dropped {initial_rows - len(df):,} rows with missing {TARGET_COL}\")\n",
        "\n",
        "# Ensure target is integer (0/1)\n",
        "df[TARGET_COL] = df[TARGET_COL].astype(int)\n",
        "print(df[TARGET_COL].value_counts(normalize=True).rename('proportion'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection and basic preprocessing\n",
        "\n",
        "# Columns that leak the target (directly derived from arrival delay)\n",
        "leakage_cols = [\n",
        "    TARGET_COL,\n",
        "    'ARR_DELAY',\n",
        "    'ARR_DELAY_NEW',\n",
        "    'ARR_DELAY_GROUP',\n",
        "]\n",
        "\n",
        "# Remove obvious identifiers that are not useful for prediction\n",
        "id_cols = ['flight_id'] if 'flight_id' in df.columns else []\n",
        "\n",
        "cols_to_drop = [c for c in leakage_cols + id_cols if c in df.columns]\n",
        "print(\"Dropping columns:\", cols_to_drop)\n",
        "\n",
        "X = df.drop(columns=cols_to_drop)\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features: {len(numeric_features)}\")\n",
        "print(f\"Categorical features: {len(categorical_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a manageable sample for modeling\n",
        "\n",
        "max_samples = 100_000  # adjust if needed\n",
        "n_rows = len(X)\n",
        "\n",
        "if n_rows > max_samples:\n",
        "    print(f\"Sampling {max_samples:,} rows out of {n_rows:,} for model training (stratified by {TARGET_COL})...\")\n",
        "    X_sample, _, y_sample, _ = train_test_split(\n",
        "        X, y, train_size=max_samples, stratify=y, random_state=42\n",
        "    )\n",
        "else:\n",
        "    print(f\"Using all {n_rows:,} rows for modeling.\")\n",
        "    X_sample, y_sample = X, y\n",
        "\n",
        "print(\"Sample class distribution:\")\n",
        "print(y_sample.value_counts(normalize=True).rename('proportion'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample,\n",
        "    y_sample,\n",
        "    test_size=0.2,\n",
        "    stratify=y_sample,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(X_train):,}, Test size: {len(X_test):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing: scale numeric features and one-hot encode categoricals\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Logistic Regression\n",
        "\n",
        "log_reg_clf = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', LogisticRegression(max_iter=200, n_jobs=None, solver='lbfgs')),\n",
        "])\n",
        "\n",
        "print(\"Training Logistic Regression model...\")\n",
        "log_reg_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = log_reg_clf.predict(X_test)\n",
        "y_proba_lr = log_reg_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nLogistic Regression Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_lr))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_lr))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_lr))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_lr))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: Random Forest\n",
        "\n",
        "rf_clf = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=None,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "    )),\n",
        "])\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "y_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nRandom Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred_rf))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
